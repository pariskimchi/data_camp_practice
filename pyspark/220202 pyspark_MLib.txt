

# Pyspark MLlib imports 


# pyspark.mllib.recommendation 
    => from pyspark.mllib.recommendation import alias

# pyspark.mllib.classification 
    => from pyspark.mllib.classification 
        import LogisticRegressionWithLBFGS
# pyspark.mllib.clustering 
    => from pyspark.mllib.clustering 
        import KMeans


# collaborative filtering 

    => finding users that share common interests 
    => used for recommender systems 

    1. User-User collaborative filtering 
        => find users that are similar to the target user 

    2. item- item collaborative filtering 
        => finds and recommends items 
        that are similar to items with the target user 

    # Rating class 
    from pyspark.mllib.recommendation import Rating 

    r = Rating(user=1, product=2, rating=5.0)
    (r[0],r[1],r[2])

    # splitting the data using randomSplit()
    data = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
    training, test = data.randomSplit([0.6, 0.4])
    training.collect()
    test.collect()


# ALS?
    => Alternanting Least Squares 
    => provides collaborative filtering 
    => ALS.train(ratings, rank, iterations)

    r1 = Rating(1,1,1.0)
    r2 = Rating(1,2,2.0)
    r3 = Rating(2,1,2.0)
    ratings = sc.parallelize([r1,r2,r3])
    ratings.collect()

    # train 
    model = ALS.train(ratings, rank=10, iterations=10)

# predictAll() - returns RDD of rating objecvts 

unrated_RDD = sc.parallelize([(1,2),(1,1)])

predictions = model.predictAll(unrated_RDD)
predictions.collect()

# Model evaluation using MSE 
    => Mse 
        => average value of the square of (
            actual rating - predicted rating
        )

rates = ratings.map(
    lambda x: ((x[0],x[1],x[2]))
)
rates.collect()

preds = predictions.map(
    lambda x: ((x[0],x[1],x[2]))
)
preds.collect()

rate_preds = rates.join(preds)
rates_preds.collect()

# MSE 
MSE = rates_preds.map(
    lambda r: r([1][0] - r[1][1])**2
).mean()



 #practice 

    # load the data into RDD 
    data= sc.textFile(file_path)

    # split the RDD 
    ratings = data.map(lambda l: l.split(','))

    # Transform the ratings RDD 
    ratings_final = ratings.map(
        lambda line: Rating(
            int(line[0]), 
            int(line[1]),
            float(line[2])
        )
    )

    # split the data into training and test 
    training_data, test_data = ratings_final.randomSplit(
        [0.8, 0.2]
    )


# practice2 

    #create the ALS model on the training data 
    model = ALS.train(
        training_data, rank=10, iterations=10
    )

    # Drop the ratings column 
    testdata_no_rating = test_data.map(
        lambda p:(p[0], p[1])
    )

    # predict the model 
    predictions = model.predictAll(testdata_no_rating)

    # return the first 2 row of the RDD 
    predictions.take(2)


# practice 3  MSE

    #prepare ratings data 
    rates = ratings_final.map(
        lambda r: ((r[0],r[1],r[2]))
    )

    # Prepare predictions data 
    preds = predictions.map(
        lambda r: ((r[0],r[1],r[2]))
    )

    # join the ratings data with prediction data 
    rates_and_preds = rates.join(preds)

    # calculate and print MSE 
    MSE = rates_and_preds.map(
        lambda r: r([1][0] - r[1][1])**2
    ).mean()

# Working with Vectors 

    # two type of vectors 
    1. Dense Vector
        => store all their entries 
            in array of floating point number 
    2. Sparse Vector 
        => store only the nonzero values and their index 

    denseVec = Vectors.dense([1.0,2.0,3.0])

    sparseVec = Vectors.sparse(4, {1:1.0, 3:5.5})


# LabelledPoint()

    => LabelledPoint
        => wrapper for input features, predicted value 

        for binary classification 
        => 0 (negative )
        => 1(positive)

    positive = LabeledPoint(1.0, [1.0, 0.0, 3.0])
    negative = LabeledPoint(0.0, [2.0, 1.0, 1.0])


# HashingTF() 
    => algorithm to map feature value to indices 
         in the feature vector 

    from pyspark.mllib.feature import HashingTF

    sentence = "hello hello world!"
    words = sentence.split()
    tf = HashingTF(10000)
    tf.transform(words) 

# Logistic Regression 

data = [
    LabeledPoint(0.0, [0.0, 1.0]),
    LabeledPoint(1.0, [1.0, 0.0])
]
RDD = sc.parallelize(data)

lrm = LogisticRegressionWithLBFGS.train(RDD)

lrm.predict([1.0, 0.0])
lrm.predict([0.0,1.0])


# Practice 4 

LR => to predict categorical response 

# spam or no spam 
    
    1. create RDD of strings email 
    2. Run MLlib's feature extraction 
        to convert text into RDD of vector 
    3. Call classification on RDD of vector 
        to return a model object to classify new point 
    4. Evaluate the model 


    # load the datasets into RDD
    spam_rdd = sc.textFile(file_path_spam)
    non_spam_rdd = sc.textFile(file_path_non_spam)

    # split the email message into words 
    spam_words= spam_rdd.flatMap(
        lambda email:email.split(' ')
    )
    non_spam_words = non_spam_rdd.flatMap(
        lambda email: email.split(' ')
    )

    # print the first element in the split RDD
    print(spam_words.first())

# Feature hasing and LabelPoint 

    after splitting the email into words, 
    raw dataset 
        => spam or non-spam 
        => 1 line message 
        => need to classify 
            => need to convert text into features 

    => 1. create HashingTF() instance 
            => to map text to vectors of 200 features. 
        2. labels for features 
            => 0 (valid) => non_spam
            => 1( non_valid)=> spam 
        
    # create a HashingTf instance with 200 features 
    tf = HashingTF(numFeatures=200)

    # Map each word to one feature 
    spam_features = tf.transform(spam_words)
    non_spam_features = tf.transform(non_spam_words)

    # Label the features 
        => 1 for spam, 0 for non-spam 
    spam_samples = spam_features.map(
        lambda features:LabeledPoint(1, features)
    )
    non_spam_samples = non_spam_features.map(
        lambda features:LabeledPoint(0, features)
    )
    
    # combine the two datasets 
    samples = spam_samples.join(non_spam_samples)


# Logistic Regression model training 

    => after creating labels and features 
        => rdy to build the model 
        => need to split train-test dataset 
    
    # split the data into training and testing 
    train_samples, test_samples = samples.randomSplit(
        [0.8, 0.2]
    )

    # train the model 
    model = LogisticRegressionWithLBFGS.train(train_samples)

    # create a prediction label from the test data 
    predictions = model.predict(
        test_samples.map(lambda x:x.features)
    )

    # combine original labels with the predicted labesl 
    labels_and_preds = test_samples.map(
        lambda x:x.label
    ).zip(predictions)

    # check the accuray of the model on the test data 
    accuracy = labels_and_preds.filter(
        lambda x:x[0] == x[1]
    ).count() / float(test_samples.count())
    print("model accuray:{:.2f}".format(accuracy))


# What is the clustering?
    => unsupervised learning task  
        to organize a collection of data into groups 

        => K-means 
        => Gaussian mixture 
        => Power iteration clustering 
        => bisecting k-means 
        => streaming k-means

    # K-means with spark MLLib 
    RDD = sc.textFile("wineData.csv").map(
        lambda x: x.split(",")
    ).map(
        lambda x: [float(x[0]), float(x[1])]
    )
    RDD.take(5)

    # train a k-means clustering model 
        => KMeans.train()

        from pyspark.mllib.clustering import KMeans 
        
        model = KMeans.train(RDD, k=2, maxIterations=10)
        model.clusterCenters

    # Evaluating the K-means model 
    from math import sqrt 
    
    def error(point):
        center = model.centers[model.predict(point)]
        return sqrt(sum([x**2 for x in (point-center)]))

    WSSSE = RDD.map(lambda point:error(point)).reduce(
        lambda x,y:x+y
    )

    # Visualizing clusters 
    wine_data_df = spark.createDataFrame(RDD, schema=["col1","col2"])
    wine_data_df_pandas = wine_data_df.toPandas()

    cluster_centers_pandas = pd.DataFrame(
        model.clusterCenters, columns=["col1","col2"]
    )
    cluster_centers_pandas.head()

    plt.scatter(wine_data_df_pandas["col1"], wine_data_df_pandas["col2"])
    plt.scatter(cluster_centers_pandas["col1"],cluster_centers_padnas["col2"],color="red")



# K-means training 

    => test with k's from 13 to 16 
    => to run k-means clustering 
        to calculate WSSSE

    # train the model with clusters 
        from 13 to 16 and compute WSSSE
    for clst in range(13, 17):
        model = KMeans.train(rdd_split_int, clst, seed=1)
        WSSSE = rdd_split_int.map(lambda point: error(point)).reduce(
            lambda x, y: x+y
        )
        