

# ML pipelines 

    import pyspark.ml 

    => spark.transform()
        => Transformer class 

    => spark.fit()
        => Estimator class 


# Data Types 

    # work on columns
    => spark.cast() 

    # work on df 
    => spark.withColumn()

    # convert Type 

    df = df.withColumn("col",df.col.cast("new_type"))

    #example 
    model_data = model_data.withColumn(
        "arr_delay",model_data.arr_delay.cast("integer")
    )

# making boolean 

    # creating is_late 
    model_data = model_data.withColun(
        "is_late",model_data.arr_delay >0
    )

    # convert to an integer 
    model_Data = model_data.withColumn(
        "label",model_data.is_late.cast("integer")
    )

    # Remove missing values 
    model_data = model_data.filter(
        "arr_delay is Not NULL and "
    )

# String and factors 

    => pyspark.ml.features 

    1. create StringIndexer 
    2. encode OneHotEncoder 

    => StringIndexer(inputcol, outputcol)
    => OneHotEncoder(
        inputCol="",outputcol=""
    )

    # practice
    carr_indexer = StringIndexer(
        inputCol="carrier",
        outputCol="carrier_index"
    )
    carr_encoder = OneHotEncoder(
        inputCol="carrier_index",
        outputCol="carrier_fact"
    )

    # VectorAssember 
        => spark.VectorAssembler()

        # example 
        vec_assembler = VectorAssembler(
            inputCols=["month", "air_time", "carrier_fact", "dest_fact", "plane_age"]
            , 
            outputCol="features"
        )

# Create the pipeline 

    => pyspark.ml 
    
    from pyspark.ml import Pipeline

    # make the pipeline 
    flights_pipe = Pipeline(
        stages=[dest_indexer,
        dest_encoder, carr_indexer, 
        carr_encoder, vec_assembler]
    )

# Transform the data 

    # fit and transform the data 
    piped_data = flights_pipe.fit(model_data).transform(model_data)

    # split the data 
        => spark.randomSplit()

        #example 
        training, test = piped_data.randomSplit([.6,.4])


# Create the modeler 

    => the Estimator => LogisticRegression

from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression()

# Cross Validation 

    => elasticNetParam 
    => regParam

    import pyspark.ml.evaluation

    => BinaryClassificationEvaluator


    # example 
    import pyspark.ml.evaluation as evals 

    # Create a BinaryClassificationEvaluator
    evaluator = evals.BinaryClassificationEvaluator(
        metricName="areaUnderROC"
    )

# make a grid 

    => pyspark.ml.tuning 

        => ParamGridBuilder 
            => .addGrid()
            => .build()


    # Practice 

    # import the tuning submodule 
    import pyspark.ml.tuning as tune 

    # Create the parameter grid 
    grid = tune.ParamGridBuilder()

    # Add the hyperparameter 
    grid = grid.addGrid(
        lr.regParam, np.arange(0, .1, .01)
    )
    grid = grid.addGrid(
        lr.elasticNetParam, [0,1]
    )

    # Build the grid 
    grid = grid.build()

# Make the validator 

    => perform cross validation to compare model 

    # Create Cross-validator 

    cv = tune.CrossValidator(
        estimator=lr,
        estimatorParamMaps=grid,
        evaluator=evaluator
    )

# Fit the model 

    # Fit cross validation models 
    models = cv.fit(training)

    # Extract the best model 
    best_lr = models.bestModel


# Evaluating binary classifiers 

    # binary classification 
        => AUC (area under curve)
        => ROC (curve)

# Evaluate the model 

    # use the model to predict the test set 
    test_results = best_lr.transform(test)

    # evaluate the predictions to compute AUC
    print(evaluator.evaluate(test_results))