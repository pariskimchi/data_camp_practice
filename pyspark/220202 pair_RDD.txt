

# Pair RDD??

# creating pair RDD?
    1. from a list , key-value tuples 
    2. from regular RDD

# Example 
    my_tuple = [('sam',23),('mary',34),('peter',25)]
    pairRDD_tuple = sc.parallelize(my_tuple)

    my_list = ['sam 23','mary 34','peter 25']
    regularRDD = sc.parallelize(my_list)
    pairRDD_RDD = regularRDD.map(lambda s: (s.split(' ')[0],s.split(' ')[1]))

# transformations on pair RDD 
    1. reduceByKey(func)
        => combine value with same key 
    2. groupByKey():
        => Group values with same key 
    3. sortByKey()
        => Return RDD sorted by the key 
    4. join()
        => join two pair RDD based on key

# reduceByKey()

    => transformations, not actions 

    regularRDD = sc.parallelize([
        ('messi',23),('ronaldo',34),
        ('neymar',22), ('messi',24)
    ])
    pairRDD_reducebykey = regularRDD.reduceByKey(
        lambda x,y: x+y
    )
    pairRDD_reducebykey.collect()

        => messi, 47
        같은 키값의 value를 lambda function으로 더해준다

# sortByKey() transformations

    pairRDD_reducebykey_rev = pairRDD_reducebykey.map(
        lambda x: (x[1],x[0])
    )
    pairRDD_reducebykey_rev.sortByKey(ascending=False).collect()

    => 
    [(47, 'messi'),(34,'ronaldo'),(22,'neymar')]

# groupByKey() transformations

    airports = [('us','jfk'),('uk','lhr'),(fr,'cdg'),
                ('us','sfo')]
    regularRDD = sc.parallelize(airports)
    pairRDD_group = regularRDD.groupByKey().collect()

    for cont, air in pairRDD_group:
        print(cont, list(air))
    ===>
    FR ['CDG']
    US ['JFK','SFO']
    UK ['LHR']
# join() transformations

    => RDD1.join(RDD2).collect()

# reduce() action 

    => reduce(func)
        => aggregating the elements 

    # ex 
    x = [1,3,4,6]
    RDD = sc.parallelize(x)
    RDD.reduce(lambda x, y: x+y)

# saveAsTextFile()

    RDD.saveAsTextFile("tempFile")

    # coalesce() => 
    RDD.coalesce(1).saveAsTextFile("tempFile")

# countByKey() action 
    => counts the number of element for each key 

    rdd = sc.parallelize([
        ("a",1), ("B",1), ("a",1)
    ])
    for kee, val in rdd.countByKey().items():
        print(kee, val)

# collectAsMap() action 

    => return key-value pairs as dict

    # ex 
    sc.parallelize([
        (1,2), (3,4)
    ]).collectAsMap()

    ==> {1:2, 3:4}

# practice 1 

    #count the unique keys 
    total = Rdd.countByKey()

    # type total?
    print(type(total))

    # iterate over the total and print the output?
    for k,v in total.items():
        print(k, v)

# practice: create base RDD 
    and transform it 

    # create a baseRDD from the file path 
    baseRDD = sc.textFile(file_path)

    # Split the lines of baseRDD into words 
    splitRDD = baseRDD.flatMap(
        lambda x:x.split()
    )

    # count the total num of words 
    print(splitRDD.count())


# practice 3 

    # convert the words in lower case and 
        remove stop words from the stop_words curated list 

    splitRDD_no_stop = splitRDD.filter(
        lambda x: x.lower() not in stop_words
    )

    # create a tuple of the word and 1 
    splitRDD_no_stop_words = splitRDD_no_stop.map(
        lambda w: (w,1)
    )

    # count of the number occurence of each word 
    resultRDD = splitRDD_no_stop_words.reduceByKey(
        lambda x, y: x+y
    )

# practice 4 

    # display first 10 words, their frequencies 
        from input RDD 
    for word in resultRDD.take(10):
        print(word)

    # swap the keys and values from the input RDD 
    resultRDD_swap = resultRDD.map(
        lambda x:(x[1],x[0])
    )

    # sort the key in descending order 
    resultRDD_swap_sort = resultRDD_swap.sortByKey(
        ascending=False
    )

    # show the top 10 most frequent words and 
        their frequencies from the sorted RDD 


